# Exploring the Relationship Between Unemployment and Consumer Spending in the United States

**Contributors:** Vedant Patel; Alex Vasilevich

## Summary 
This project investigates how monthly U.S. unemployment levels relate to household consumer spending (Personal Consumption Expenditures, PCE) over a thirty-five year window. The motivating question is whether labor market stress shows up promptly and meaningfully in aggregate spending, a linkage that matters to policy makers, forecasters, and businesses planning inventory or staffing. We focus on authoritative, public-domain sources to keep the analysis transparent: the Civilian Unemployment Rate (UNRATE) from the Bureau of Labor Statistics and the PCE series from the Bureau of Economic Analysis, both accessed through the Federal Reserve Economic Data (FRED) API. The timeframe is January 1990 through December 2024, matching what is currently available in the raw downloads.

The workflow is fully scripted. Data acquisition pulls both series via HTTPS from the FRED observations endpoint using the `requests` library, with the API key passed through the `FRED_API_KEY` environment variable. Files are written to `data/raw/` as `unrate_raw.csv` and `pce_raw.csv`, and the pipeline is orchestrated by `scripts/run_all.py` or by Snakemake (`workflow/Snakefile`). A small amount of deterministic cleaning aligns column names, parses dates to monthly frequency, coerces numeric fields, and performs an inner join on the `date` column. No imputations or adjustments are necessary because both series arrive complete and seasonally adjusted; inflation adjustment is already baked into the chained-dollar PCE series.

After cleaning, the merged dataset lives at `data/processed/unemployment_pce_merged.csv` with 420 monthly rows covering 1990-01-01 through 2024-12-01 and zero missing values. Exploratory analysis summarizes the series and produces four visuals saved to `outputs/`: `trends_over_time.png`, `correlation_scatter.png`, and `change_scatter.png` for pattern spotting, plus `eda_summary.txt` for descriptive statistics. The summary confirms minimum and maximum unemployment rates of 3.4% and 14.8%, PCE ranging from roughly $3.7T to $20.5T (billions, chained), and a Pearson correlation of -0.2286 between the two level series.

Modeling remains deliberately simple to stay interpretable. A levels regression, `PCE ~ Unemployment`, estimated with 420 observations, yields an R-squared of 0.052. The coefficient on unemployment is -567.84 (p < 0.001), meaning a one percentage-point increase in unemployment is associated with an average $568B decrease in monthly PCE over the sample period. A second specification that adds a one-month lag of unemployment retains a similar R-squared (0.053), but both the contemporaneous (-534.36, p = 0.154) and lagged (-36.95, p = 0.921) coefficients lose statistical significance, implying little immediate predictive lead once the current rate is known.

The evidence fits the visual story: spending pulled back sharply during the 2008 financial crisis and the 2020 COVID-19 shock, the same months when unemployment spiked. At the same time, the low R-squared and residual autocorrelation indicate substantial unexplained variation driven by omitted macro factors such as incomes, transfer payments, prices, and interest rates. Rather than over-claim, we frame the results as a baseline that isolates the bivariate relationship and quantifies its modest strength. All outputs, including regression diagnostics (`modeling_summary.txt`, `modeling_coefficients.csv`, `predicted_vs_actual.png`, and `residuals_vs_fitted.png`), are regenerated from scratch by the automated pipeline, ensuring reproducibility and auditability.

## Data Profile 
Both inputs come from FRED so that access, licensing, and documentation are consistent. UNRATE (Civilian Unemployment Rate, seasonally adjusted, percent) is published by the Bureau of Labor Statistics and distributed by the Federal Reserve Bank of St. Louis. PCE (Personal Consumption Expenditures, billions of chained dollars, seasonally adjusted) is published by the Bureau of Economic Analysis and likewise served by FRED. The FRED series identifiers are `UNRATE` and `PCE`. Because FRED republishes U.S. government statistics, both are public-domain, redistributable datasets with no personal information.

Data acquisition uses the FRED observations endpoint and requires only HTTPS requests and an API key stored in `FRED_API_KEY`. The scripts call the endpoint once per series, parameterized by the start and end dates used throughout the repository: 1990-01-01 to 2024-12-31 for the request window, producing observations through 2024-12-01, which is the latest available monthly point. Response payloads are parsed into two CSVs placed in `data/raw/`: `unrate_raw.csv` and `pce_raw.csv`. The raw files retain FRED metadata columns such as `date` and `value` and are written without manual editing.

The cleaning and integration stage standardizes column names, parses the date string into a monthly datetime, coerces numeric values to floats, and performs an inner join on `date` so both series share the same timeline. Because the FRED source is complete, no rows require imputation or dropping for missingness. The merged file, `data/processed/unemployment_pce_merged.csv`, contains 420 rows and three columns (`date`, `unrate`, `pce`) spanning 1990-01-01 to 2024-12-01 with zero missing values, as documented in `data/processed/data_profile.txt`.

Storage layout follows a simple, inspectable structure: raw downloads in `data/raw/`, processed data in `data/processed/`, and plots/model outputs in `outputs/`. The repo includes a `data/box/` directory reserved for any large or embargoed artifacts, but it is gitignored and currently empty; Box sharing instructions live in this README so the link can be added later without committing private files. Repository-level documentation also notes the environment files (`.env` and `.env.example`) and `requirements.txt`, which pins libraries such as pandas, requests, seaborn, matplotlib, statsmodels, and snakemake.

Metadata and provenance are tracked in plain text to keep the project auditable. The `data/processed/data_profile.txt` file records the row count, date span, and missingness check. Outputs from the modeling and EDA steps—`eda_summary.txt` and `modeling_summary.txt`—capture summary statistics and full regression tables so the numeric claims in this README can be traced back to generated artifacts. All scripts are deterministic; rerunning acquisition and cleaning with the same date parameters yields identical row counts and values, ensuring reproducibility without manual intervention.

Ethical and legal considerations are minimal because the data involve aggregate macroeconomic indicators, not individual-level records. Nonetheless, we keep the FRED API key out of version control via `.gitignore` and provide `.env.example` so collaborators can configure credentials safely. No terms of use are violated by redistribution of these particular series, and the methodology avoids scraping or bypassing any access controls.

## Data Quality 
Completeness is strong because both series are official economic indicators with long histories and stable publication schedules. The merged dataset contains 420 observations, one for each month between January 1990 and December 2024, and the missingness checks recorded in `data/processed/data_profile.txt` confirm zero nulls for both `unrate` and `pce`. No rows were dropped during integration because the inner join found matching dates across the entire window, reflecting aligned calendars for the two series.

Consistency checks focused on date parsing, numeric coercion, and duplicate detection. Dates are converted to pandas monthly timestamps, ensuring uniform frequency for time-series operations. The numeric value fields from FRED are coerced to floats; any non-numeric anomalies would have surfaced as parsing errors, but none were observed. A de-duplication pass verified that each month appears exactly once in the merged table, preventing accidental double-counting. Column naming (`unrate`, `pce`) is standardized to avoid confusion with the raw `value` labels.

Validity assessments looked at ranges and distributions relative to known economic behavior. The unemployment rate ranges from 3.4% to 14.8%, consistent with historical extremes during tight labor markets and the 2020 spike. PCE ranges from roughly $3.7T to $20.5T (billions of chained dollars), reflecting long-run growth and pandemic-era volatility. Descriptive statistics in `outputs/eda_summary.txt` show reasonable means (unemployment 5.71%, PCE $9.79T) and standard deviations, with no out-of-bounds values. Outlier handling was intentionally conservative: no values were winsorized or removed because the peaks correspond to real macro shocks.

Provenance and reproducibility are addressed through code-driven steps and stored artifacts. Acquisition scripts are deterministic; given the same `start-date` and `end-date` parameters, they request the same FRED observations and write identical CSVs. Cleaning and integration are likewise scripted and log-free of manual edits. The presence of `data/processed/data_profile.txt`, `outputs/eda_summary.txt`, and `outputs/modeling_summary.txt` allows reviewers to verify that claimed counts, ranges, and model statistics match the generated files. Because directories are created automatically by the Snakemake workflow, reruns on clean systems do not fail due to missing folders.

Remaining limitations stem from the scope of variables rather than the cleanliness of the two included series. With only unemployment and PCE, the bivariate setup cannot control for confounders like personal income, transfer payments, prices, or interest rates, which means residuals exhibit autocorrelation and low explanatory power. Additionally, while PCE is already adjusted for inflation, the analysis does not explore alternative transformations (logs or differences) that might better stabilize variance or interpretability. These limitations are documented so readers understand that the primary risks are model under-specification, not data quality issues.

## Findings 
Visual inspection of the two series shows parallel narratives during major shocks. In `trends_over_time.png`, PCE grows steadily through most of the sample but dips sharply during the 2008 financial crisis and the 2020 COVID-19 onset; unemployment spikes in those same periods, creating clear inverse co-movements. Outside of crises, spending growth appears smoother than unemployment, hinting that other stabilizing forces support consumption even when joblessness rises modestly.

The static correlation between levels is modestly negative at -0.2286, as reported in `outputs/eda_summary.txt` and visualized in `correlation_scatter.png`. The scatterplot shows a downward trend with noticeable curvature: points cluster tightly at lower unemployment rates with high spending, while crisis periods introduce more dispersion. A change-to-change view in `change_scatter.png` also shows negative alignment, suggesting that months with rising unemployment often coincide with softer spending growth, though the pattern remains noisy.

Regression analysis quantifies these relationships while keeping the specification intentionally simple. The levels model `PCE ~ Unemployment` uses 420 observations and yields an R-squared of 0.052. The coefficient on unemployment is -567.84 with a p-value of 2.21e-06, so the estimate is statistically significant despite the low explanatory power. Interpreted economically, a one percentage-point increase in unemployment is associated with an average $568B decrease in monthly PCE in chained dollars over 1990-2024. The intercept of about $13.0T aligns with the average spending level when unemployment sits near its mean.

To test whether unemployment has short-term predictive content beyond contemporaneous alignment, a second model adds a one-month lag of unemployment. The R-squared nudges only to 0.053, and both the contemporaneous coefficient (-534.36, p = 0.154) and the lagged coefficient (-36.95, p = 0.921) are statistically insignificant. This result suggests that, in this bivariate setup, current unemployment explains what little variation the model captures, and recent unemployment history contributes almost nothing once the current rate is known.

Diagnostic plots reinforce the limitations. `predicted_vs_actual.png` shows substantial under-fit during extreme events: the model misses the steep PCE drops around 2008 and 2020, leading to large residuals that are also visible in `residuals_vs_fitted.png`. The Durbin-Watson statistic in `modeling_summary.txt` is effectively zero, indicating strong residual autocorrelation and signaling that time-series methods or richer covariates are needed. Taken together, the findings support a modest inverse relationship but also make clear that unemployment alone cannot account for most of the variation in consumer spending.

## Future Work 
Expanding the variable set is the most direct way to improve explanatory power. Incorporating measures of personal income, transfer payments, consumer prices, and interest rates would allow multivariate regressions that separate the effects of labor market slack from other macro forces. FRED offers suitable series (for example DPI for income, PCEPI or CPI for prices, and FEDFUNDS for rates), and the existing acquisition script can be extended with additional parameters and a wider schema. With richer inputs, we could test whether unemployment remains significant after controlling for disposable income and price levels.

Transformations and time-series structure deserve attention to address autocorrelation and scale. Taking logarithms of PCE and unemployment, or differencing the series, may stabilize variance and convert coefficients into elasticities that are easier to interpret across regimes. Distributed lag models or ARIMAX/VECM approaches could explicitly model persistence and feedback between spending and labor market conditions. These steps would also help check whether the modest negative correlation in levels holds up under alternative, stationarity-friendly specifications.

Event and regime analysis could clarify how the relationship behaves under stress. Structural break tests around the 2008 crisis and the 2020 pandemic would reveal whether the slope between unemployment and PCE shifts during extreme periods. Rolling-window regressions could show whether the sensitivity of spending to unemployment has changed across expansions and recessions. If breaks are detected, a piecewise or regime-switching model might capture non-linear dynamics better than a single global coefficient.

Data quality extensions include documenting and storing version hashes for raw downloads to ensure that future reruns capture the same vintages, especially if FRED revises historical values. Adding automated checks that assert expected row counts (420), date ranges (1990-01-01 to 2024-12-01), and missingness (zero) would make the pipeline fail fast if upstream data shift. Similarly, writing unit tests for the acquisition and cleaning functions could catch schema changes in API responses.

Reproducibility and distribution can be strengthened by containerizing the project and publishing an archive with metadata. A Zenodo deposit with the reproducible code, environment lockfile, and generated outputs would provide a citable DOI. Finally, documenting a Box link in the `data/box/` section once large artifacts exist will keep big files out of git while remaining accessible to collaborators.

## Reproducing
1. Python 3.10+ and pip installed.
2. `python -m venv .venv && .\\.venv\\Scripts\\activate` (Windows) or `source .venv/bin/activate` (macOS/Linux).
3. `pip install -r requirements.txt`.
4. Export your FRED key: Windows `setx FRED_API_KEY your_key`; macOS/Linux `export FRED_API_KEY=your_key`. (Or copy `.env.example` to `.env` and load manually.)
5. Run everything: `python scripts/run_all.py`.
   - Or with Snakemake: `snakemake -s workflow/Snakefile -j1`.
6. Outputs: raw data in `data/raw/`; merged data in `data/processed/unemployment_pce_merged.csv`; plots and model summaries in `outputs/`.
7. Box data: place any large or embargoed outputs in `data/box/` and share via Box (provide link here: **[insert Box link]**). Do not commit Box contents.

## References
- Bureau of Labor Statistics, Civilian Unemployment Rate (UNRATE) [Data set]. FRED, Federal Reserve Bank of St. Louis. https://fred.stlouisfed.org/series/UNRATE
- Bureau of Economic Analysis, Personal Consumption Expenditures (PCE) [Data set]. FRED, Federal Reserve Bank of St. Louis. https://fred.stlouisfed.org/series/PCE
- Python packages: pandas, requests, seaborn, matplotlib, statsmodels, snakemake (see `requirements.txt`).

